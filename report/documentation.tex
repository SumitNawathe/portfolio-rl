\chapter{Documentation}

\section{Gym Environments}

WIP

\section{RL Framework}

In order to ensure consistency and modularity, we have created a framework of classes
that allows us to easily swap components of our RL environment to test various data
and reward strategies.

\subsection{AbstractRewardManager}

This abstract class manages the calculation of rewards for the RL agent.
Its interface is minimal. Subclasses can optionally have a constructor.

\begin{minted}[escapeinside=||,mathescape=true,linenos,xleftmargin=20pt]{python3}
from abc import ABC, abstractmethod

class AbstractRewardManager(ABC):
  @abstractmethod
  def initialize_reward(self):
    """
    Initializes all constants used in the reward calculation.
    Must be called at least once before compute_reward() is called.
    Should be called in the environment's reset() method.
    """
    pass

  @abstractmethod
  def compute_reward(self, old_pval: float, new_pval: float) -> float:
    """
    Computes the reward based on the old and new portfolio values.
    Should be called in the environment's step() method.
    """
    pass
\end{minted}


\subsection{AbstractDataManager}

This abstract class manages reading and iterating through the state and price data for the environment.

\begin{minted}[escapeinside=||,mathescape=true,linenos,xleftmargin=20pt]{python3}
from abc import ABC, abstractmethod
import numpy as np
import numpy.typing as npt
import gymnasium as gym

class AbstractDataManager(ABC):
  @abstractmethod
  def get_obs_space(self) -> gym.spaces.Box:
    """Result is assigned to the environment's observation space"""
    pass

  @abstractmethod
  def get_data(self) -> tuple[int, int]:
    """
    This function loads/fetches state data from files and stores it.
    Should be called in the environment's reset() method.
    The properties assigned here should be accessed in get_state().
    Note that the data should provide for one more than the number
      of time periods desired (for the initial state).
    Returns: (number of time periods, number of stock tickers)
    """
    pass

  @abstractmethod
  def get_state(
    self,
    t: int,
    w: npt.NDArray[np.float64],
    port_val: np.float64
  ) -> npt.NDArray[np.float64]:
    """
    Computes and returns the new state at time t.
    State can include the current portfolio weight and value,
      provided as additional parameters.
    This state will be used by the agent for calculating weights
      at the start time time period t+1.
    When t=0, it should output the initial state.
    """
    pass

  @abstractmethod
  def get_prices(self, t: int) -> npt.NDArray[np.float64]:
    """
    Returns the security prices at time t (at the beginning
      of time period t+1).
    When t=0, it should output the initial prices.
    """
    pass
  \end{minted}

\subsection{PortfolioEnvWithTCost}

This class is the main gym environment.
It iterates through the data and rewards using the \texttt{AbstractDataManager} and \texttt{AbstractRewardManager} provided as arguments.
To compute the portfolio updates, it employs the transaction costs-cognizant approach described in section \ref{transaction_costs_section}.

Let \texttt{w} be the portfolio weights. We use the convention that \texttt{w[-1]} is the weight for the risk-free asset,
and \texttt{w[:-1]} are the weights for the risky assets. (Note that this is a different convention than the one used in \cite{drl_framework}.)

We omit much of the actual code and instead provide pseudocode or equations where relevant.

\begin{minted}[escapeinside=||,mathescape=true,linenos,xleftmargin=20pt]{python3}
from abc import ABC, abstractmethod
import numpy as np
import numpy.typing as npt
import gymnasium as gym

class PortfolioEnvWithTCost(gym.Env):
  def __init__(
    self,
    dm: AbstractDataManager,
    rm: AbstractRewardManager,
    w_lb=0.0, w_ub=1.0,
    cp=0.0, cs=0.0,
    logging=True
  ):
    """
    Initializes the environment with the given managers and constants.
    w_lb and w_ub are the lower and upper bounds for the portfolio weights.
    cp and cs are commision weights for purchasing and selling assets.
    If logging is enabled, step() will return the portfolio value.
    """
    # register managers
    # set constants from given arguments

    # get data from manager
    self.num_time_periods, self.universe_size = self.dm.get_data()

    # set environment observation and action spaces
    assert w_lb <= w_ub
    self.observation_space = self.dm.get_obs_space()
    self.action_space = gym.spaces.Box(
      low=w_lb,
      high=w_ub,
      shape=(self.universe_size + 1,),
      dtype=np.float64
    )

  def find_mu(
    self,
    w_old: npt.NDArray[np.float64],
    w_new = npt.NDArray[np.float64]
  ) -> float:
    """
    Uses the iterative technique to find mu for the given old weights
    (after returns), new weights, and the transaction cost commision rates.
    """

def step(self, action: npt.NDArray[np.float64]) -> tuple:
  """
  Performs one time step update using the agent's action.
  Returns: a tuple of
    - copy of the new state
    - reward for the action
    - whether the environment has terminated (reached end of data)
    - whether the environment has truncated (always False)
    - information dictionary (possibly containing portfolio value)
  """
  # check that the action is normalized
  # find the value of mu using the helper function
  # perform the weight and portfolio value updates
  # obtain the reward uding the reward manager
  # obtain the new state using the data manager
  # update all instance variables
  # if logging enabled, return the portfolio value in addition to

def reset(self, *args, **kwargs) -> tuple[np.ndarray, dict]:
  """
  Resets the environment to an initial state.
  Returns: the initial state and an empty dictionary (required by gym).
  """
  # reset portfolio weights
  # initialize reward manager
  # obtain initial state and prices from data manager
    # portfolio weights (final is cash weight)
    self.w = np.zeros(self.universe_size + 1, dtype=float)
    self.w[-1] = 1.0

    self.port_val = 1.0

    self.rm.initialize_reward()

    # compute and return initial state
    self.t = 0
    self.state = self.dm.get_state(self.t, self.w, self.port_val)
    self.v = self.dm.get_prices(self.t)
    return self.state.copy(), {}
\end{minted}
