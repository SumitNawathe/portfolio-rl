
@inproceedings{maps,
	title = {{MAPS}: {Multi}-agent {Reinforcement} {Learning}-based {Portfolio} {Management} {System}},
	shorttitle = {{MAPS}},
	url = {http://arxiv.org/abs/2007.05402},
	doi = {10.24963/ijcai.2020/623},
	abstract = {Generating an investment strategy using advanced deep learning methods in stock markets has recently been a topic of interest. Most existing deep learning methods focus on proposing an optimal model or network architecture by maximizing return. However, these models often fail to consider and adapt to the continuously changing market conditions. In this paper, we propose the Multi-Agent reinforcement learning-based Portfolio management System (MAPS). MAPS is a cooperative system in which each agent is an independent "investor" creating its own portfolio. In the training procedure, each agent is guided to act as diversely as possible while maximizing its own return with a carefully designed loss function. As a result, MAPS as a system ends up with a diversified portfolio. Experiment results with 12 years of US market data show that MAPS outperforms most of the baselines in terms of Sharpe ratio. Furthermore, our results show that adding more agents to our system would allow us to get a higher Sharpe ratio by lowering risk with a more diversified portfolio.},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Lee, Jinho and Kim, Raehyun and Yi, Seok-Won and Kang, Jaewoo},
	month = jul,
	year = {2020},
	note = {arXiv:2007.05402 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Multiagent Systems},
	pages = {4520--4526},
	annote = {Comment: 7 pages, 5 figures, IJCAI-PRICAI 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\sumit\\Zotero\\storage\\7HA5ESIF\\Lee et al. - 2020 - MAPS Multi-agent Reinforcement Learning-based Por.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumit\\Zotero\\storage\\YBMHP64Z\\2007.html:text/html},
}

@article{learn_to_rank,
	series = {Machine learning in finance},
	title = {Stock portfolio selection using learning-to-rank algorithms with news sentiment},
	volume = {264},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231217311098},
	doi = {10.1016/j.neucom.2017.02.097},
	abstract = {In this study, we apply learning-to-rank algorithms to design trading strategies using relative performance of a group of stocks based on investors’ sentiment toward these stocks. We show that learning-to-rank algorithms are effective in producing reliable rankings of the best and the worst performing stocks based on investors’ sentiment. More specifically, we use the sentiment shock and trend indicators introduced in the previous studies, and we design stock selection rules of holding long positions of the top 25\% stocks and short positions of the bottom 25\% stocks according to rankings produced by learning-to-rank algorithms. We then apply two learning-to-rank algorithms, ListNet and RankNet, in stock selection processes and test long-only and long-short portfolio selection strategies using 10 years of market and news sentiment data. Through backtesting of these strategies from 2006 to 2014, we demonstrate that our portfolio strategies produce risk-adjusted returns superior to the S\&P 500 index return, the hedge fund industry average performance - HFRIEMN, and some sentiment-based approaches without learning-to-rank algorithm during the same period.},
	urldate = {2024-03-29},
	journal = {Neurocomputing},
	author = {Song, Qiang and Liu, Anqi and Yang, Steve Y.},
	month = nov,
	year = {2017},
	keywords = {Financial news sentiment, Learning-to-rank, Long-short strategy, Stock portfolio selection, Trading strategy},
	pages = {20--28},
	file = {Accepted Version:C\:\\Users\\sumit\\Zotero\\storage\\X36L5ZYZ\\Song et al. - 2017 - Stock portfolio selection using learning-to-rank a.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\sumit\\Zotero\\storage\\4G477YSK\\S0925231217311098.html:text/html},
}

@misc{drl_framework,
	title = {A {Deep} {Reinforcement} {Learning} {Framework} for the {Financial} {Portfolio} {Management} {Problem}},
	url = {http://arxiv.org/abs/1706.10059},
	abstract = {Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25\% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
	month = jul,
	year = {2017},
	note = {arXiv:1706.10059 [cs, q-fin]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Finance - Computational Finance, Quantitative Finance - Portfolio Management},
	annote = {Comment: 30 pages, 5 figures, submitting to JMLR},
	file = {arXiv.org Snapshot:C\:\\Users\\sumit\\Zotero\\storage\\7ER5M7KI\\1706.html:text/html;Full Text PDF:C\:\\Users\\sumit\\Zotero\\storage\\MLGBNVX7\\Jiang et al. - 2017 - A Deep Reinforcement Learning Framework for the Fi.pdf:application/pdf},
}

@misc{rl_augmented_states,
	title = {Reinforcement-{Learning} based {Portfolio} {Management} with {Augmented} {Asset} {Movement} {Prediction} {States}},
	url = {http://arxiv.org/abs/2002.05780},
	doi = {10.48550/arXiv.2002.05780},
	abstract = {Portfolio management (PM) is a fundamental financial planning task that aims to achieve investment goals such as maximal profits or minimal risks. Its decision process involves continuous derivation of valuable information from various data sources and sequential decision optimization, which is a prospective research direction for reinforcement learning (RL). In this paper, we propose SARL, a novel State-Augmented RL framework for PM. Our framework aims to address two unique challenges in financial PM: (1) data heterogeneity -- the collected information for each asset is usually diverse, noisy and imbalanced (e.g., news articles); and (2) environment uncertainty -- the financial market is versatile and non-stationary. To incorporate heterogeneous data and enhance robustness against environment uncertainty, our SARL augments the asset information with their price movement prediction as additional states, where the prediction can be solely based on financial data (e.g., asset prices) or derived from alternative sources such as news. Experiments on two real-world datasets, (i) Bitcoin market and (ii) HighTech stock market with 7-year Reuters news articles, validate the effectiveness of SARL over existing PM approaches, both in terms of accumulated profits and risk-adjusted profits. Moreover, extensive simulations are conducted to demonstrate the importance of our proposed state augmentation, providing new insights and boosting performance significantly over standard RL-based PM method and other baselines.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Ye, Yunan and Pei, Hengzhi and Wang, Boxin and Chen, Pin-Yu and Zhu, Yada and Xiao, Jun and Li, Bo},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05780 [cs, q-fin, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Portfolio Management, Statistics - Machine Learning},
	annote = {Comment: AAAI 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\sumit\\Zotero\\storage\\LJ398S44\\Ye et al. - 2020 - Reinforcement-Learning based Portfolio Management .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumit\\Zotero\\storage\\5Y4TCJUV\\2002.html:text/html},
}

@article{drl_modern_portfolio_theory,
	title = {Deep reinforcement learning for stock portfolio optimization by connecting with modern portfolio theory},
	volume = {218},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742300057X},
	doi = {10.1016/j.eswa.2023.119556},
	abstract = {With artificial intelligence and data quality development, portfolio optimization has improved rapidly. Traditionally, researchers in the financial market have utilized the modern portfolio theory for portfolio optimization; however, with the recent development of artificial intelligence, attempts to optimize portfolios with reinforcement learning are increasing. Many studies have developed reinforcement learning and deep learning algorithms and conducted portfolio optimization research. However, in reality, thus far, the securities industry thus has used the modern portfolio theory, which is sufficiently valuable. Nevertheless, to the best of our knowledge, there has yet to be an attempt to combine modern portfolio theory and reinforcement learning. To bridge this gap in the literature, we propose a novel deep reinforcement learning approach that combines the modern portfolio theory and a deep learning approach. As far as we know, we are the first to combine recent deep learning technology and traditional financial theory. Specifically, we solved the multimodal problem through the Tucker decomposition of a model with the input of technical analysis and stock return covariates. The results show that the proposed method outperforms state-of-the-art algorithms regarding the Sharpe ratio, annualized return, and maximum drawdown. In addition, the proposed method dynamically changes the weight according to the market trend, unlike other state-of-the-art algorithms.},
	urldate = {2024-03-29},
	journal = {Expert Systems with Applications},
	author = {Jang, Junkyu and Seong, NohYoon},
	month = may,
	year = {2023},
	keywords = {Deep reinforcement learning, Modern portfolio theory, Portfolio management, Technical analysis, Tensor decomposition},
	pages = {119556},
	file = {ScienceDirect Snapshot:C\:\\Users\\sumit\\Zotero\\storage\\JRMTX4B7\\S095741742300057X.html:text/html},
}

@article{drl_mvo,
	title = {Deep {Reinforcement} {Learning} for {Optimal} {Portfolio} {Allocation}: {A} {Comparative} {Study} with {Mean}-{Variance} {Optimization}},
	abstract = {Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals and objectives. Portfolio Optimization is a key component that involves allocating the portfolio assets so as to maximize returns while minimizing risk taken. Portfolio optimization is typically carried out by financial professionals who use a combination of quantitative techniques and investment expertise to make decisions about the portfolio allocation.},
	language = {en},
	author = {Sood, Srijan and Papasotiriou, Kassiani and Vaiciulis, Marius and Balch, Tucker},
	file = {Sood et al. - Deep Reinforcement Learning for Optimal Portfolio .pdf:C\:\\Users\\sumit\\Zotero\\storage\\K4DH7CRS\\Sood et al. - Deep Reinforcement Learning for Optimal Portfolio .pdf:application/pdf},
}

@inproceedings{finbert,
	address = {New York, NY, USA},
	series = {{ICAIF} '23},
	title = {{FinBERT}-{FOMC}: {Fine}-{Tuned} {FinBERT} {Model} with {Sentiment} {Focus} {Method} for {Enhancing} {Sentiment} {Analysis} of {FOMC} {Minutes}},
	isbn = {9798400702402},
	shorttitle = {{FinBERT}-{FOMC}},
	url = {https://dl.acm.org/doi/10.1145/3604237.3626843},
	doi = {10.1145/3604237.3626843},
	abstract = {In this research project, we used the financial texts published by the Federal Open Market Committee (FOMC), known as the FOMC Minutes, for sentiment analysis. The pre-trained FinBERT model, a state-of-the-art transformer-based model trained for NLP tasks in finance, was utilized for that. The focus of this research has been on improving the predictive performance of complex financial sentences, as our problem analysis has shown that such sentences pose a significant challenge to existing models. To accomplish this objective the original FinBERT model was fine-tuned for domain-specific sentiment analysis. A strategy, referred to as Sentiment Focus (SF) was utilized to reduce the complexity of sentences, making them more amenable to accurate sentiment predictions. To evaluate the efficacy of our method, we curated a manually labeled test dataset comprising 1375 entries. The results demonstrated an overall improvement of in accuracy when using SF-enhanced fine-tuned FinBERT over the original FinBERT model. In cases of complex sentences containing conjunctions like but, while, and though with contradicting sentiments, our fine-tuned model outperformed the original FinBERT by a margin of .},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the {Fourth} {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Gössi, Sandro and Chen, Ziwei and Kim, Wonseong and Bermeitinger, Bernhard and Handschuh, Siegfried},
	month = nov,
	year = {2023},
	keywords = {Economics, Financial Economics, FinBERT, FOMC Minutes, Natural Language Processing, Sentiment Analysis, Sentiment Focus},
	pages = {357--364},
	file = {Full Text PDF:C\:\\Users\\sumit\\Zotero\\storage\\TP9G8SN2\\Gössi et al. - 2023 - FinBERT-FOMC Fine-Tuned FinBERT Model with Sentim.pdf:application/pdf},
}

@incollection{diff_sharpe_ratio_book,
	location = {Boston, {MA}},
	title = {Reinforcement Learning for Trading Systems and Portfolios: Immediate vs Future Rewards},
	isbn = {978-1-4615-5625-1},
	url = {https://doi.org/10.1007/978-1-4615-5625-1_10},
	shorttitle = {Reinforcement Learning for Trading Systems and Portfolios},
	abstract = {We propose to train trading systems and portfolios by optimizing financial objective functions via reinforcement learning. The performance functions that we consider as value functions are profit or wealth, the Sharpe ratio and our recently proposed differential Sharpe ratio for online learning. In Moody \& Wu (1997), we presented empirical results in controlled experiments that demonstrated the efficacy of some of our methods for optimizing trading systems. Here we extend our previous work to the use of Q-Learning, a reinforcement learning technique that uses approximated future rewards to choose actions, and compare its performance to that of our previous systems which are trained to maximize immediate reward. We also provide new simulation results that demonstrate the presence of predictability in the monthly S\&P 500 Stock Index for the 25 year period 1970 through 1994.},
	pages = {129--140},
	booktitle = {Decision Technologies for Computational Finance: Proceedings of the fifth International Conference Computational Finance},
	publisher = {Springer {US}},
	author = {Moody, John and Saffell, Matthew and Liao, Yuansong and Wu, Lizhong},
	editor = {Refenes, Apostolos-Paul N. and Burgess, Andrew N. and Moody, John E.},
	urldate = {2024-04-08},
	date = {1998},
	langid = {english},
	doi = {10.1007/978-1-4615-5625-1_10},
	file = {Full Text PDF:C\:\\Users\\sumit\\Zotero\\storage\\RT9THVZH\\Moody et al. - 1998 - Reinforcement Learning for Trading Systems and Por.pdf:application/pdf},
}


@inproceedings{diff_sharpe_ratio_paper,
	title = {Optimization of trading systems and portfolios},
	url = {https://ieeexplore.ieee.org/document/618952},
	doi = {10.1109/CIFER.1997.618952},
	abstract = {We propose to train trading systems and portfolios by optimizing objective functions that directly measure trading and investment performance. Rather than basing a trading system on forecasts or training via a supervised learning algorithm using labelled trading data, we train our systems using recurrent reinforcement learning algorithms. The objective functions that we consider as evaluation functions for reinforcement learning are profit or wealth, economic utility, the Sharpe ratio, and our proposed Differential Sharpe Ratio. The trading and portfolio management systems require prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. This temporal dependence on system state requires the use of reinforcement versions of standard recurrent learning algorithms. We present empirical results in controlled experiments that demonstrate the efficacy of some of our methods. We find that maximizing the differential Sharpe ratio yields more consistent results than maximizing profits, and that both methods outperform a trading system based on forecasts that minimize {MSE}.},
	eventtitle = {Proceedings of the {IEEE}/{IAFE} 1997 Computational Intelligence for Financial Engineering ({CIFEr})},
	pages = {300--307},
	booktitle = {Proceedings of the {IEEE}/{IAFE} 1997 Computational Intelligence for Financial Engineering ({CIFEr})},
	author = {Moody, J. and Wu, Lizhong},
	urldate = {2024-04-08},
	date = {1997-03},
	keywords = {Backpropagation algorithms, Costs, Economic forecasting, Finance, Investments, Labeling, Management training, Optimization methods, Portfolios, Supervised learning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sumit\\Zotero\\storage\\SILKYD86\\618952.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sumit\\Zotero\\storage\\LYEUZIJG\\Moody and Wu - 1997 - Optimization of trading systems and portfolios.pdf:application/pdf},
}


