{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy.typing as npt\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from importlib import reload\n",
    "\n",
    "from portfolio_env_framework import *\n",
    "from data_utils import *\n",
    "from rewards import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-1/5))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2010) & (df.date.dt.year <= 2017)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[(news_df.date.dt.year >= 2010) & (news_df.date.dt.year <= 2017)]\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-1/5))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2018) & (df.date.dt.year <= 2019)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        print(news_sentiment_array)\n",
    "\n",
    "        # read index data and compute volatilities\n",
    "        idx_df = pd.read_csv('crsp_snpidx_2010_to_2024.csv', dtype={\n",
    "          'DATE': 'string',\n",
    "          'vwretd': float\n",
    "        })\n",
    "        idx_df.DATE = pd.to_datetime(idx_df.DATE)\n",
    "        idx_df['vol_20'] = idx_df.vwretd.rolling(20).std()\n",
    "        idx_df['vol_60'] = idx_df.vwretd.rolling(60).std()\n",
    "        idx_df.set_index('DATE', inplace=True)\n",
    "        self.idx_df = idx_df\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_EIEE_CNN_Extractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 1067):\n",
    "        super(Custom_EIEE_CNN_Extractor, self).__init__(observation_space, features_dim)\n",
    "        n_channels, self.universe_size, data_len = observation_space['data'].shape\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 6, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(6, 8, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 10, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 12, kernel_size=(1, data_len-6)),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.cnn(observations['data'])\n",
    "        return torch.cat((x.flatten(start_dim=1), observations['weights'].flatten(start_dim=1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.universe_size=82\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravipanguluri/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/stable_baselines3/common/buffers.py:605: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 10.90GB > 4.64GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m DDPG(\u001b[39m'\u001b[39m\u001b[39mMultiInputPolicy\u001b[39m\u001b[39m'\u001b[39m, train_env, buffer_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, policy_kwargs\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mfeatures_extractor_class\u001b[39m\u001b[39m'\u001b[39m: Custom_EIEE_CNN_Extractor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m }, action_noise\u001b[39m=\u001b[39mNormalActionNoise(mean\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, sigma\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mones(\u001b[39m83\u001b[39m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m10\u001b[39;49m, log_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ravipanguluri/Documents/portfolio-rl/finalized_news_tests.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# model.save(\"cnn_portoflio_policy\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    124\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    125\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    126\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    127\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    128\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    129\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[39mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    226\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    227\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    228\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    229\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    349\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:188\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m# Optimize the critics\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 188\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    191\u001b[0m \u001b[39m# Delayed policy updates\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/portfolio-rl/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_env = PortfolioEnvWithTCost(dm=TrainDataManager(), rm=DifferentialSharpeRatioReward(), cp=0.01, cs=0.01)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "train_env.action_space.seed(43)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = DDPG('MultiInputPolicy', train_env, buffer_size=4*10**5, verbose=1, policy_kwargs={\n",
    "  'features_extractor_class': Custom_EIEE_CNN_Extractor,\n",
    "}, action_noise=NormalActionNoise(mean=0, sigma=0.05*np.ones(83)))\n",
    "model.learn(total_timesteps=10**10, log_interval=1)\n",
    "# model.save(\"cnn_portoflio_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(PortfolioEnvWithTCost, n_envs=1, env_kwargs={\n",
    "    'dm': TestDataManager(),\n",
    "    'rm': DifferentialSharpeRatioReward(),\n",
    "    'cp': 0.10/365,\n",
    "    'cs': 0.10/365\n",
    "})\n",
    "obs, _ = env.reset()\n",
    "\n",
    "port_val = [1.0]\n",
    "snp_val = [1.0]\n",
    "actions = []\n",
    "ys = []\n",
    "times_arr = [env.dm.times[env.t+15]]\n",
    "mus = []\n",
    "\n",
    "for i in range(900):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    actions.append(action)\n",
    "    # print(f\"external {action=}\")\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    ys.append(env.y)\n",
    "    mus.append(env.mu)\n",
    "    times_arr.append(env.dm.times[env.t+15])\n",
    "    port_val.append(info['port_val'])\n",
    "    snp_val.append(snp_val[-1] * (1 + env.dm.idx_df[env.dm.idx_df.index == env.dm.times[env.t+15]].vwretd.values[0]))\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-2/5))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2010) & (df.date.dt.year <= 2017)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-2/5))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2018) & (df.date.dt.year <= 2019)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        print(news_sentiment_array)\n",
    "\n",
    "        # read index data and compute volatilities\n",
    "        idx_df = pd.read_csv('crsp_snpidx_2010_to_2024.csv', dtype={\n",
    "          'DATE': 'string',\n",
    "          'vwretd': float\n",
    "        })\n",
    "        idx_df.DATE = pd.to_datetime(idx_df.DATE)\n",
    "        idx_df['vol_20'] = idx_df.vwretd.rolling(20).std()\n",
    "        idx_df['vol_60'] = idx_df.vwretd.rolling(60).std()\n",
    "        idx_df.set_index('DATE', inplace=True)\n",
    "        self.idx_df = idx_df\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PortfolioEnvWithTCost(dm=TrainDataManager(), rm=DifferentialSharpeRatioReward(), cp=0.01, cs=0.01)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "train_env.action_space.seed(43)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = DDPG('MultiInputPolicy', train_env, buffer_size=4*10**5, verbose=1, policy_kwargs={\n",
    "  'features_extractor_class': Custom_EIEE_CNN_Extractor,\n",
    "}, action_noise=NormalActionNoise(mean=0, sigma=0.05*np.ones(83)))\n",
    "model.learn(total_timesteps=10**10, log_interval=1)\n",
    "# model.save(\"cnn_portoflio_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(PortfolioEnvWithTCost, n_envs=1, env_kwargs={\n",
    "    'dm': TestDataManager(),\n",
    "    'rm': DifferentialSharpeRatioReward(),\n",
    "    'cp': 0.10/365,\n",
    "    'cs': 0.10/365\n",
    "})\n",
    "obs, _ = env.reset()\n",
    "\n",
    "port_val = [1.0]\n",
    "snp_val = [1.0]\n",
    "actions = []\n",
    "ys = []\n",
    "times_arr = [env.dm.times[env.t+15]]\n",
    "mus = []\n",
    "\n",
    "for i in range(900):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    actions.append(action)\n",
    "    # print(f\"external {action=}\")\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    ys.append(env.y)\n",
    "    mus.append(env.mu)\n",
    "    times_arr.append(env.dm.times[env.t+15])\n",
    "    port_val.append(info['port_val'])\n",
    "    snp_val.append(snp_val[-1] * (1 + env.dm.idx_df[env.dm.idx_df.index == env.dm.times[env.t+15]].vwretd.values[0]))\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-1/10))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2010) & (df.date.dt.year <= 2017)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[(news_df.date.dt.year >= 2010) & (news_df.date.dt.year <= 2017)]\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataManager(AbstractDataManager):\n",
    "    def get_obs_space(self) -> gym.spaces.Box:\n",
    "        return gym.spaces.Dict({\n",
    "            'data': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4, self.universe_size, 10), dtype=np.float32),\n",
    "            'weights': gym.spaces.Box(low=0, high=1, shape=(self.universe_size+1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    def get_data(self) -> tuple[int, int]:\n",
    "        # read all data\n",
    "        df = read_crsp_data()\n",
    "        news_df = read_news_data(pd.date_range(df.date.min(), df.date.max(), freq='D'), gamma=(1-1/10))\n",
    "\n",
    "    \n",
    "        # only include stocks that are present in all dates and in both datasets\n",
    "        df_ticker_ok = df.TICKER.value_counts() == df.TICKER.value_counts().max()\n",
    "        def is_max_val_count(ticker: str) -> bool:\n",
    "          return df_ticker_ok[ticker] and (ticker not in ['GOOG', 'EXC'])\n",
    "        df = df[df.apply(lambda row: is_max_val_count(row['TICKER']), axis=1)]\n",
    "        df = df[(df.date.dt.year >= 2018) & (df.date.dt.year <= 2019)]\n",
    "        self.times = df.date.unique()[1:]\n",
    "        self.tickers = df.TICKER.unique()\n",
    "        news_df = news_df[news_df.apply(lambda row: (row.ticker in self.tickers), axis=1)]\n",
    "    \n",
    "        # create stock array\n",
    "        self.stock_df = df.pivot(index='date', columns='TICKER', values='PRC').astype(float)\n",
    "        self.high_df = df.pivot(index='date', columns='TICKER', values='ASKHI').astype(float)\n",
    "        self.low_df = df.pivot(index='date', columns='TICKER', values='BIDLO').astype(float)\n",
    "        \n",
    "        # adjust for stock splits\n",
    "        facpr_df = df.pivot(index='date', columns='TICKER', values='FACPR').astype(float)\n",
    "        self.stock_df = self.stock_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.high_df = self.high_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.low_df = self.low_df * (1+facpr_df).cumprod(axis=0)\n",
    "        self.ret = np.log(self.stock_df.pct_change().iloc[1:, :] + 1)\n",
    "        \n",
    "        # create pivot tables\n",
    "        news_sentiment_array = news_df.pivot(index='date', columns='ticker', values= 'sentiment_embedding').astype(float)\n",
    "        no_sentiment_tickers = list(set(self.tickers) - set(news_sentiment_array.columns.values))\n",
    "\n",
    "        news_sentiment_array[[no_sentiment_tickers]] = 0\n",
    "        self.news_sent_df = news_sentiment_array\n",
    "        print(news_sentiment_array)\n",
    "\n",
    "        # read index data and compute volatilities\n",
    "        idx_df = pd.read_csv('crsp_snpidx_2010_to_2024.csv', dtype={\n",
    "          'DATE': 'string',\n",
    "          'vwretd': float\n",
    "        })\n",
    "        idx_df.DATE = pd.to_datetime(idx_df.DATE)\n",
    "        idx_df['vol_20'] = idx_df.vwretd.rolling(20).std()\n",
    "        idx_df['vol_60'] = idx_df.vwretd.rolling(60).std()\n",
    "        idx_df.set_index('DATE', inplace=True)\n",
    "        self.idx_df = idx_df\n",
    "        \n",
    "        self.num_time_periods = len(self.times)-15-1\n",
    "        self.universe_size = len(self.tickers)\n",
    "        print(f\"{self.universe_size=}\")\n",
    "        return self.num_time_periods, self.universe_size\n",
    "    \n",
    "    def get_state(self, t: int, w: npt.NDArray[np.float64], port_val: np.float64) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        s = np.zeros((4, self.universe_size, 10))\n",
    "        s[0, :, :] = self.stock_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[1, :, :] = self.high_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[2, :, :] = self.low_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        s[3, :, : ] = self.news_sent_df.loc[self.times[t:t+10], self.tickers].to_numpy().T\n",
    "        return {'data': s, 'weights': w}\n",
    "\n",
    "    def get_prices(self, t: int) -> npt.NDArray[np.float64]:\n",
    "        # today is self.times[self.t+10]\n",
    "        return np.append(self.stock_df.loc[self.times[t+10], :].to_numpy().flatten(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PortfolioEnvWithTCost(dm=TrainDataManager(), rm=DifferentialSharpeRatioReward(), cp=0.01, cs=0.01)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "train_env.action_space.seed(43)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = DDPG('MultiInputPolicy', train_env, buffer_size=4*10**5, verbose=1, policy_kwargs={\n",
    "  'features_extractor_class': Custom_EIEE_CNN_Extractor,\n",
    "}, action_noise=NormalActionNoise(mean=0, sigma=0.05*np.ones(83)))\n",
    "model.learn(total_timesteps=10**10, log_interval=1)\n",
    "# model.save(\"cnn_portoflio_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(PortfolioEnvWithTCost, n_envs=1, env_kwargs={\n",
    "    'dm': TestDataManager(),\n",
    "    'rm': DifferentialSharpeRatioReward(),\n",
    "    'cp': 0.10/365,\n",
    "    'cs': 0.10/365\n",
    "})\n",
    "obs, _ = env.reset()\n",
    "\n",
    "port_val = [1.0]\n",
    "snp_val = [1.0]\n",
    "actions = []\n",
    "ys = []\n",
    "times_arr = [env.dm.times[env.t+15]]\n",
    "mus = []\n",
    "\n",
    "for i in range(900):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    actions.append(action)\n",
    "    # print(f\"external {action=}\")\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    ys.append(env.y)\n",
    "    mus.append(env.mu)\n",
    "    times_arr.append(env.dm.times[env.t+15])\n",
    "    port_val.append(info['port_val'])\n",
    "    snp_val.append(snp_val[-1] * (1 + env.dm.idx_df[env.dm.idx_df.index == env.dm.times[env.t+15]].vwretd.values[0]))\n",
    "    if terminated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
